{
 "metadata": {
  "name": "",
  "signature": "sha256:167e1546e8adf011926067b2e275e5e7dfb82b8f1b70e8028b30ba598b2836e0"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "News n' Stocks"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Testing prediction power of news articles terms against stock prices."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Connect to Mongo server."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pymongo import MongoClient\n",
      "client = MongoClient('mongodb://localhost:27017/')\n",
      "db = client['news']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Find entire range of daily summaries available."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "period_summaries = db['period_summary']\n",
      "query = period_summaries.aggregate([{\n",
      "      '$group': {\n",
      "        '_id': 'date_range',\n",
      "        'min_date': { '$min': '$period_start' },\n",
      "        'max_date': { '$max': '$period_end' }\n",
      "      }\n",
      "    }]).next()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "start_date = query['min_date']\n",
      "end_date = query['max_date']\n",
      "print start_date, end_date, end_date - start_date"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2013-01-01 00:00:00 2014-10-19 00:00:00 656 days, 0:00:00\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Save some of the dates to use as validation."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "base_date = start_date + (end_date - start_date) * 3/4\n",
      "print base_date"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2014-05-08 00:00:00\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import datetime\n",
      "test_dates = [base_date + datetime.timedelta(days=x) for x in range(0, (end_date - base_date).days + 1)]\n",
      "print test_dates[0], test_dates[len(test_dates)-1], len(test_dates)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2014-05-08 00:00:00 2014-10-19 00:00:00 165\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Get list of stock quotes from Yahoo!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from stock_downloader import download_stock_history\n",
      "\n",
      "stock_list = ['IBM', 'XOM', 'CVX', 'PG', 'MMM', 'JNJ', 'MCD', 'WMT', 'UTX', 'KO', 'BA', 'CAT', 'JPM', 'HPQ', 'VZ', 'T', 'DD',\n",
      "              'MRK', 'DIS', 'HD', 'MSFT', 'AXP', 'BAC', 'PFE', 'GE', 'INTC', 'AA', 'C', 'GM']\n",
      "\n",
      "stock_data_panel = download_stock_history(start_date - datetime.timedelta(days=10), end_date + datetime.timedelta(days=1), stock_list, source='yahoo')\n",
      "print stock_data_panel"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<class 'pandas.core.panel.Panel'>\n",
        "Dimensions: 6 (items) x 459 (major_axis) x 29 (minor_axis)\n",
        "Items axis: Open to Adj Close\n",
        "Major_axis axis: 2012-12-24 00:00:00 to 2014-10-20 00:00:00\n",
        "Minor_axis axis: AA to XOM\n"
       ]
      }
     ],
     "prompt_number": 158
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Re-index our data to extend to all possible days (even those with no trading day), interpolate missing days values and get pandas DataFrame with just the 'Close' price of the stocks."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "date_index = pd.DatetimeIndex(start=start_date, end=end_date + datetime.timedelta(days=1), freq='D')\n",
      "stock_data_panel = stock_data_panel.reindex(major_axis=date_index)\n",
      "stock_data = stock_data_panel['Close'].interpolate().dropna()\n",
      "print stock_data.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "                  AA        AXP         BA        BAC          C        CAT  \\\n",
        "2013-01-02  8.990000  58.750000  77.070000  12.030000  41.250000  93.500000   \n",
        "2013-01-03  9.070000  59.000000  77.470001  11.960000  41.389999  94.400002   \n",
        "2013-01-04  9.260000  59.610001  77.690002  12.110000  42.430000  94.919998   \n",
        "2013-01-05  9.206667  59.693334  77.170000  12.103333  42.443334  95.016665   \n",
        "2013-01-06  9.153333  59.776668  76.649999  12.096667  42.456667  95.113332   \n",
        "\n",
        "                   CVX         DD        DIS         GE    ...            MMM  \\\n",
        "2013-01-02  110.389999  45.869998  51.099998  21.340000    ...      94.779999   \n",
        "2013-01-03  109.919998  45.290001  51.209999  21.100000    ...      94.669998   \n",
        "2013-01-04  110.500000  45.729998  52.189999  21.200001    ...      95.370003   \n",
        "2013-01-05  110.250000  45.753332  51.783333  21.176667    ...      95.410001   \n",
        "2013-01-06  110.000000  45.776666  51.376667  21.153333    ...      95.450000   \n",
        "\n",
        "                  MRK       MSFT        PFE         PG          T        UTX  \\\n",
        "2013-01-02  41.340000  27.620001  25.910000  69.389999  35.000000  84.000000   \n",
        "2013-01-03  42.330002  27.250000  25.850000  68.949997  35.020000  84.309998   \n",
        "2013-01-04  41.970001  26.740000  25.959999  69.089996  35.230000  84.980003   \n",
        "2013-01-05  42.020000  26.723334  25.966666  68.933332  35.283333  84.843335   \n",
        "2013-01-06  42.070000  26.706667  25.973333  68.776667  35.336666  84.706668   \n",
        "\n",
        "                   VZ        WMT        XOM  \n",
        "2013-01-02  44.270000  69.239998  88.709999  \n",
        "2013-01-03  44.060001  68.800003  88.550003  \n",
        "2013-01-04  44.299999  69.059998  88.959999  \n",
        "2013-01-05  44.429999  68.839999  88.616666  \n",
        "2013-01-06  44.559999  68.620001  88.273333  \n",
        "\n",
        "[5 rows x 29 columns]\n"
       ]
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Get top 1,000 most relevant terms from base_date and try to correlate to stock prices via granger test."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "daily_summary = period_summaries.find_one({\n",
      "        'period_type' : { '$eq' : 'daily' },\n",
      "        'period_end' : { '$eq' : base_date }\n",
      "    })\n",
      "yearly_summary = period_summaries.find_one({\n",
      "        'period_type' : { '$eq' : 'yearly' },\n",
      "        'period_end' : { '$eq' : base_date }\n",
      "    })\n",
      "print daily_summary['total_docs'], daily_summary['total_terms'], len(daily_summary['term_counts'])\n",
      "print yearly_summary['total_docs'], yearly_summary['total_terms'], len(yearly_summary['term_counts'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "98.0 6566.0 1000\n",
        "26640.0 2116940.0 1000\n"
       ]
      }
     ],
     "prompt_number": 38
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Find daily relevant terms and intersect with yearly terms to see if daily terms are most likely noise from a higher time frame."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "daily_relevant_terms = map(lambda x : x[0], daily_summary['term_counts'])\n",
      "#yearly_relevant_terms = map(lambda x : x[0], yearly_summary['term_counts'])\n",
      "print daily_relevant_terms[:10]\n",
      "#print yearly_relevant_terms[:10]\n",
      "#print len(list(set(daily_relevant_terms).intersection(yearly_relevant_terms)))\n",
      "#print list(set(daily_relevant_terms).difference(yearly_relevant_terms))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[u'bank', u'china', u'company', u'water', u'chinese', u'bitcoin', u'scout', u'percent', u'council', u'vietnam']\n"
       ]
      }
     ],
     "prompt_number": 39
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from statsmodels.tsa.stattools import grangercausalitytests\n",
      "import numpy as np"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 40
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "daily_summaries = db['daily_summary']\n",
      "from news_calc_term_ts import get_tfidf_ts\n",
      "one_term_ts = get_tfidf_ts(daily_summaries, [daily_relevant_terms[0]], {})\n",
      "print one_term_ts"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<class 'pandas.core.panel.Panel'>\n",
        "Dimensions: 1 (items) x 656 (major_axis) x 3 (minor_axis)\n",
        "Items axis: bank to bank\n",
        "Major_axis axis: 2013-01-01 00:00:00 to 2014-10-19 00:00:00\n",
        "Minor_axis axis: Count to TF-IDF Score\n"
       ]
      }
     ],
     "prompt_number": 41
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from kpss import kpssTest\n",
      "import matplotlib, matplotlib.pyplot as plt\n",
      "matplotlib.style.use('ggplot')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 104
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "term_df = one_term_ts[one_term_ts.items[0]]['TF-IDF Score']\n",
      "stock_df = stock_data[['AA']]['AA']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 139
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def difference(ts, n=1):\n",
      "    return (ts - ts.shift(n)).dropna()\n",
      "\n",
      "def is_stationary(kpss_result, alpha=0.05):\n",
      "    return kpss_result[1] > alpha\n",
      "\n",
      "def find_diff_stationary(df, max_diff=5):\n",
      "    i = 0\n",
      "    while i < max_diff:\n",
      "        if is_stationary(kpssTest(np.asarray(df))):\n",
      "            return (i, df, True)\n",
      "        else:\n",
      "            df = difference(df)\n",
      "        i += 1\n",
      "    return (i, df, False)   \n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 154
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "diff_level, new_df, converged = find_diff_stationary(term_df)\n",
      "print diff_level, converged\n",
      "#term_df_diff = difference(term_df)\n",
      "#term_ts_kpss = kpssTest(np.asarray(term_df_diff), \"LEVEL\")\n",
      "#stock_df_diff = difference(stock_df)\n",
      "#stock_ts_kpss = kpssTest(np.asarray(stock_df_diff), \"LEVEL\")\n",
      "#kpssTest(np.random.randn(1000))\n",
      "#one_term_ts[one_term_ts.items[0]]['TF-IDF Score'].plot()\n",
      "#stock_df_diff.plot()\n",
      "#plt.show()\n",
      "#stock_data[['AA']].plot()\n",
      "#plt.show()\n",
      "#print 'term: ', term_ts_kpss, ' stock: ', stock_ts_kpss"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "KPSS Test for  LEVEL  Stationarity\n",
        "\n",
        "KPSS LEVEL=0.617829\n",
        "Truncation lag parameter=5\n",
        "p-value=0.021016\n",
        "KPSS Test for  LEVEL  Stationarity\n",
        "\n",
        "KPSS LEVEL=0.016838\n",
        "Truncation lag parameter=5\n",
        "p-value=0.100000\n",
        "\n",
        "Warning: p-value greater than printed p-value\n",
        "1 True\n"
       ]
      }
     ],
     "prompt_number": 157
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#r = grangercausalitytests(np.asarray(stock_data), 1)\n",
      "#np.asarray(stock_data).shape\n",
      "#test = one_term_ts.concat(stock_data[['AA']])\n",
      "#r = grangercausalitytests(stock_data[['AA', 'AXP']], 5)\n",
      "#pd.concat([one_term_ts[one_term_ts.items[0]], stock_data[['AA']]])\n",
      "#pd.merge(stock_data[['AA']], one_term_ts[one_term_ts.items[0]])\n",
      "#type(stock_data[['AA']])\n",
      "test = pd.concat([one_term_ts[one_term_ts.items[0]], stock_data[['AA']]], axis=1).dropna()\n",
      "r = grangercausalitytests(test[['TF-IDF Score', 'AA']], 5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Granger Causality\n",
        "('number of lags (no zero)', 1)\n",
        "ssr based F test:         F=1.1876  , p=0.2762  , df_denom=651, df_num=1\n",
        "ssr based chi2 test:   chi2=1.1931  , p=0.2747  , df=1\n",
        "likelihood ratio test: chi2=1.1920  , p=0.2749  , df=1\n",
        "parameter F test:         F=1.1876  , p=0.2762  , df_denom=651, df_num=1\n",
        "\n",
        "Granger Causality\n",
        "('number of lags (no zero)', 2)\n",
        "ssr based F test:         F=3.8743  , p=0.0213  , df_denom=648, df_num=2\n",
        "ssr based chi2 test:   chi2=7.8084  , p=0.0202  , df=2\n",
        "likelihood ratio test: chi2=7.7621  , p=0.0206  , df=2\n",
        "parameter F test:         F=3.8743  , p=0.0213  , df_denom=648, df_num=2\n",
        "\n",
        "Granger Causality\n",
        "('number of lags (no zero)', 3)\n",
        "ssr based F test:         F=2.9188  , p=0.0335  , df_denom=645, df_num=3\n",
        "ssr based chi2 test:   chi2=8.8516  , p=0.0313  , df=3\n",
        "likelihood ratio test: chi2=8.7920  , p=0.0322  , df=3\n",
        "parameter F test:         F=2.9188  , p=0.0335  , df_denom=645, df_num=3\n",
        "\n",
        "Granger Causality\n",
        "('number of lags (no zero)', 4)\n",
        "ssr based F test:         F=2.6575  , p=0.0320  , df_denom=642, df_num=4\n",
        "ssr based chi2 test:   chi2=10.7791 , p=0.0292  , df=4\n",
        "likelihood ratio test: chi2=10.6909 , p=0.0303  , df=4\n",
        "parameter F test:         F=2.6575  , p=0.0320  , df_denom=642, df_num=4\n",
        "\n",
        "Granger Causality\n",
        "('number of lags (no zero)', 5)\n",
        "ssr based F test:         F=2.6708  , p=0.0212  , df_denom=639, df_num=5\n",
        "ssr based chi2 test:   chi2=13.5837 , p=0.0185  , df=5\n",
        "likelihood ratio test: chi2=13.4437 , p=0.0196  , df=5\n",
        "parameter F test:         F=2.6708  , p=0.0212  , df_denom=639, df_num=5\n"
       ]
      }
     ],
     "prompt_number": 75
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x=np.random.randn(1000)  #   is level stationary    \n",
      "kpssTest(x)\n",
      "\n",
      "y=np.cumsum(x)           # has unit root    \n",
      "kpssTest(y)\n",
      "\n",
      "z=x+0.3*np.arange(1,len(x)+1)   # is trend stationary\n",
      "kpssTest(z,\"TREND\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "KPSS Test for  LEVEL  Stationarity\n",
        "\n",
        "KPSS LEVEL=0.295897\n",
        "Truncation lag parameter=7\n",
        "p-value=0.100000\n",
        "\n",
        "Warning: p-value greater than printed p-value\n",
        "KPSS Test for  LEVEL  Stationarity\n",
        "\n",
        "KPSS LEVEL=5.203732\n",
        "Truncation lag parameter=7\n",
        "p-value=0.010000\n",
        "\n",
        "Warning: p-value smaller than printed p-value\n",
        "KPSS Test for  TREND  Stationarity\n",
        "\n",
        "KPSS TREND=0.113630\n",
        "Truncation lag parameter=7\n",
        "p-value=0.100000\n",
        "\n",
        "Warning: p-value greater than printed p-value\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 135,
       "text": [
        "(0.11362987151678511, 0.1, 7)"
       ]
      }
     ],
     "prompt_number": 135
    }
   ],
   "metadata": {}
  }
 ]
}